{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adaptation with GRL\n",
    "\n",
    "In this lab, we will implement the paper [_Unsupervised Domain Adaptation by Backpropagation_](https://arxiv.org/abs/1409.7495) which adapts between two domains using a Gradient Reversal Layer.\n",
    "\n",
    "Our source domain will be **MNIST**, while our target domain will be **MNIST-M**. It is a modified version of MNIST with various color.\n",
    "\n",
    "### Loading data\n",
    "\n",
    "We will use only a subset of the datasets to be faster, but feel free to use all the datasets if you have a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "USE_SUBSET = True\n",
    "\n",
    "def get_subset(x, y):\n",
    "    if not USE_SUBSET:\n",
    "        return x, y\n",
    "\n",
    "    subset_index = 10000\n",
    "    np.random.seed(1)\n",
    "    indexes = np.random.permutation(len(x))[:subset_index]\n",
    "    x, y = x[indexes], y[indexes]\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading source dataset MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from skimage.color import gray2rgb\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x_source_train, y_source_train), (x_source_test, y_source_test) = mnist.load_data()\n",
    "\n",
    "def process_mnist(x):\n",
    "    x = np.moveaxis(x, 0, -1)\n",
    "    x = resize(x, (32, 32), anti_aliasing=True, mode='constant')\n",
    "    x = np.moveaxis(x, -1, 0)\n",
    "    return gray2rgb(x).astype(\"float32\")\n",
    "\n",
    "x_source_train = process_mnist(x_source_train)\n",
    "x_source_test = process_mnist(x_source_test)\n",
    "\n",
    "x_source_train, y_source_train = get_subset(x_source_train, y_source_train)\n",
    "#x_source_test, y_source_test = get_subset(x_source_test, y_source_test)\n",
    "\n",
    "x_source_train, x_source_val, y_source_train, y_source_val = train_test_split(\n",
    "    x_source_train, y_source_train,\n",
    "    test_size=int(0.1 * len(x_source_train))\n",
    ")\n",
    "\n",
    "x_source_train.shape, x_source_val.shape, x_source_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, digit in enumerate(np.unique(y_source_train), start=1):\n",
    "    index = np.where(y_source_train == digit)[0][0]\n",
    "    ax = plt.subplot(1, 10, i)\n",
    "    ax.imshow(x_source_train[index])\n",
    "    ax.set_title(digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading target dataset MNIST-M:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"mnistm_data.pkl\", \"rb\") as f:\n",
    "    mnist_m = pkl.load(f)\n",
    "    \n",
    "x_target_train, y_target_train = get_subset(mnist_m[\"x_train\"], mnist_m[\"y_train\"])\n",
    "x_target_test, y_target_test = mnist_m[\"x_test\"], mnist_m[\"y_test\"]\n",
    "\n",
    "x_target_train = resize(x_target_train, (x_target_train.shape[0], 32, 32, 3), anti_aliasing=True, mode='edge').astype(\"float32\")\n",
    "x_target_test = resize(x_target_test, (x_target_test.shape[0], 32, 32, 3), anti_aliasing=True, mode='edge').astype(\"float32\")\n",
    "\n",
    "x_target_train.shape, x_target_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "for i, digit in enumerate(np.unique(y_target_train), start=1):\n",
    "    index = np.where(y_target_train == digit)[0][0]\n",
    "    ax = plt.subplot(1, 10, i)\n",
    "    ax.imshow(x_target_train[index])\n",
    "    ax.set_title(digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive model\n",
    "\n",
    "In the first step, we will build a naive model, depicted in the image below. Implement it as shown:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnn archi](images/cnn_archi_nogrl.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MaxPool2D, Conv2D, Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_network(input_shape=x_source_train.shape[1:]):\n",
    "    # TODO\n",
    "    return Model(inputs=inputs, outputs=digits_classifier)\n",
    "\n",
    "\n",
    "model = get_network()\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=SGD(lr=0.1, momentum=0.9, nesterov=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/da_naive_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x_source_train, y_source_train,\n",
    "    validation_data=(x_source_val, y_source_val),\n",
    "    epochs=10,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training on our **source dataset MNIST**, we evaluate our model performance on both the **source (MNIST)** and the **target dataset MNIST-M**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss & Accuracy on MNIST test set:\")\n",
    "model.evaluate(x_source_test, y_source_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss & Accuracy on MNIST-M test set:\")\n",
    "model.evaluate(x_target_test, y_target_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the two datasets are too different. The model didn't generalize on the target set.\n",
    "\n",
    "### Model with Gradient Reversal Layer\n",
    "\n",
    "![cnn archi](images/cnn_archi.png)\n",
    "\n",
    "Let us first define a Gradient Rerversal Layer where we want to inverse the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def grad_reverse(x):\n",
    "    y = tf.identity(x)\n",
    "    def custom_grad(dy):\n",
    "        return None # TODO\n",
    "    return y, custom_grad\n",
    "\n",
    "class GradReverse(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"grl\")\n",
    "\n",
    "    def call(self, x):\n",
    "        return grad_reverse(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/grl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define the whole model: convnet + classification branch + domain branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adaptable_network(input_shape=x_source_train.shape[1:]):\n",
    "    # TODO\n",
    "    return Model(inputs=inputs, outputs=None)\n",
    "\n",
    "model = get_adaptable_network()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/da_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our generators. Note that we also add the domain labels. We choose arbitrarily to set the source domain to 1, and the target domain to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "d_source_train = np.ones_like(y_source_train)\n",
    "d_source_val = np.ones_like(y_source_val)\n",
    "\n",
    "source_train_generator = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_source_train, y_source_train, d_source_train)).batch(batch_size)\n",
    "\n",
    "d_target_train = np.zeros_like(y_target_train)\n",
    "\n",
    "target_train_generator = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_target_train, d_target_train)\n",
    ").batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train alternatively on the source and target dataset. Fill the following block. \n",
    "\n",
    "**Note that to work properly we set a low factor of 0.2 to the domain losses**.\n",
    "\n",
    "See the documentation for more information on how to use GradientTape: [doc](https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough#define_the_loss_and_gradient_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Mean, Accuracy\n",
    "\n",
    "\n",
    "optimizer = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "cce = SparseCategoricalCrossentropy()\n",
    "bce = BinaryCrossentropy()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=[cce, bce],\n",
    "    metrics=[\"accuracy\", \"accuracy\"]\n",
    ")\n",
    "\n",
    "def train_epoch(source_train_generator, target_train_generator):\n",
    "    global lambda_factor, global_step\n",
    "\n",
    "    # Keras provide helpful classes to monitor various metrics:\n",
    "    epoch_source_digits = tf.keras.metrics.Mean()\n",
    "    epoch_source_domains = tf.keras.metrics.Mean()\n",
    "    epoch_target_domains = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    # Fetch all trainable variables but those used uniquely for the digits classification:\n",
    "    variables_but_classifier = list(filter(lambda x: \"digits\" not in x.name, model.trainable_variables))\n",
    "    \n",
    "    loss_record = collections.defaultdict(list)\n",
    "    \n",
    "    for i, data in enumerate(zip(source_train_generator, target_train_generator)):\n",
    "        source_data, target_data = data\n",
    "        # Training digits classifier & domain classifier on source:\n",
    "        x_source, y_source, d_source = source_data\n",
    "\n",
    "        # Remember that you can do forward likewise:\n",
    "        #   outputs = model(inputs)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # TODO\n",
    "\n",
    "        gradients = tape.gradient(# TODO, # TODO)\n",
    "        optimizer.apply_gradients(zip(# TODO, # TODO))\n",
    "\n",
    "        # Training domain classifier on target:\n",
    "        x_target, d_target = target_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            # TODO\n",
    "\n",
    "        gradients = tape.gradient(# TODO, # TODO)\n",
    "        optimizer.apply_gradients(zip(# TODO, # TODO))\n",
    "\n",
    "        # Log the various losses and accuracy\n",
    "        epoch_source_digits(digits_loss)\n",
    "        epoch_source_domains(domains_loss)\n",
    "        epoch_accuracy(y_source, digits_prob)\n",
    "        epoch_target_domains(target_loss)\n",
    "\n",
    "    print(\"Source digits loss={}, Source Accuracy={}, Source domain loss={}, Target domain loss={}\".format(\n",
    "        epoch_source_digits.result(), epoch_accuracy.result(), \n",
    "        epoch_source_domains.result(), epoch_target_domains.result()))\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: {}\".format(epoch), end=\" \")\n",
    "    loss_record = train_epoch(source_train_generator, target_train_generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new model has more metrics & losses than the previous one. To know what they are we can display the `metrics_name`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance on both source and target dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss & Accuracy on MNIST test set:\")\n",
    "model.evaluate(x_source_test, [y_source_test, np.ones_like(y_source_test)], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss & Accuracy on MNIST-M test set:\")\n",
    "model.evaluate(x_target_test, [y_target_test, np.zeros_like(y_target_test)], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is still not as good on the target dataset (MNIST-M) than on the source dataset (MNIST), but the performance are much better!\n",
    "Without using target labels we improve our performance from 40% of accuracy to more than 60% of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homework**\n",
    "\n",
    "- Train on the whole dataset\n",
    "- Train for more epochs, use callbacks such as EarlyStopping to know when to stop\n",
    "- Try to improve model by scheduling the learning rate as they do in the paper\n",
    "- Try to improve model by scheduling the domain loss weight\n",
    "- Try others domains, like SVHN -> MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
